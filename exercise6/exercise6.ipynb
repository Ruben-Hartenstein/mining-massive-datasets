{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining Massive Datasets Problem Set 6\n",
    "\n",
    "Ruben Hartenstein, Taha Erkoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jaccard similarity for two sets $C_1, C_2$ is defined as:\n",
    "\n",
    "$sim(C_1, C_2) = \\frac{|C_1 \\cup C_2|}{|C_1 \\cap C_2|}$\n",
    "\n",
    "For multi-sets, where an element can be a member more than once, the definition of the intersection and union needs to account for the multiplicities of the elements.\n",
    "\n",
    "The intersection of two multi-sets $C_1, C_2$ is defined as:\n",
    "\n",
    "$(C_1 \\cap C_2)(x) = min(C_1(x), C_2(x))$\n",
    "\n",
    "where $C_1(x), C_2(x)$ represent the counts of elements x in these two sets respectively.\n",
    "\n",
    "Therefore the union of two multi-sets can also be defined:\n",
    "\n",
    "$(C_1 \\cup C_2)(x) = max(C_1(x), C_2(x))$\n",
    "\n",
    "\n",
    "Using these two definitions, we can define the Jaccard similarity for multi-sets as:\n",
    "\n",
    "$sim(C_1, C_2) = \\frac{\\sum_{x \\in C_1 \\cup C_2} min(C_1(x), C_2(x))}{\\sum_{x \\in C_1 \\cup C_2} max(C_1(x), C_2(x))}$\n",
    "\n",
    "With this definition, in the case of two sets where each element appears at most one, the formula reduced back to its original form:\n",
    "\n",
    "$sim(C_1, C_2) = \\frac{|C_1 \\cup C_2|}{|C_1 \\cap C_2|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lit, input_file_name, concat_ws, collect_list, size, array_distinct\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"CharacterShingling\").getOrCreate()\n",
    "path = \"/grundgesetz/brd_grundgesetz_63_2019-04-03.txt\"\n",
    "\n",
    "# Load all text files from folder into a DataFrame\n",
    "documents = spark.read.text(path).withColumnRenamed(\"value\", \"text\")\n",
    "\n",
    "# Add column for file name\n",
    "documents = documents.withColumn(\"file_name\", input_file_name())\n",
    "\n",
    "# Group documents by file name and concatenate to single block of text\n",
    "documents_grouped = documents.groupBy(\"file_name\").agg(\n",
    "    concat_ws(\" \", collect_list(\"text\")).alias(\"full_text\")\n",
    ")\n",
    "\n",
    "\n",
    "# Function to generate shingles of size k from a text\n",
    "def generate_shingles(text, k):\n",
    "    shingles = set()\n",
    "\n",
    "    # Handle line breaks, hyphens and remove extra spaces\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"- \", \"\").replace(\"\\r\", \" \")\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    # Generate shingles\n",
    "    for i in range(len(text) - k + 1):\n",
    "        shingle = text[i:i + k]\n",
    "        shingles.add(shingle)\n",
    "\n",
    "    return list(shingles)\n",
    "\n",
    "\n",
    "# Register the function as a UDF\n",
    "shingles_udf = udf(lambda text, k: generate_shingles(text, k), ArrayType(StringType()))\n",
    "\n",
    "# Generate shingles for k = 5 and k = 9\n",
    "results = {}\n",
    "for k in [5, 9]:\n",
    "    # Add column for shingles of size k\n",
    "    shingles_df = documents_grouped.withColumn(f\"shingles_{k}\", shingles_udf(col(\"full_text\"), lit(k)))\n",
    "\n",
    "    # Add column for the number of distinct shingles\n",
    "    shingles_count_df = shingles_df.withColumn(f\"distinct_shingles_{k}\", size(array_distinct(col(f\"shingles_{k}\"))))\n",
    "\n",
    "    # Add results to the dictionary\n",
    "    results[k] = shingles_count_df.select(\"file_name\", f\"distinct_shingles_{k}\")\n",
    "\n",
    "    # Filter results for the Grundgesetz\n",
    "    grundgesetz_results = results[k].filter(col(\"file_name\").contains(\"grundgesetz\"))\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nResults for k={k}:\")\n",
    "    results[k].show(truncate=False)\n",
    "\n",
    "    print(f\"\\nGrundgesetz results for k={k}:\")\n",
    "    grundgesetz_results.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "Hash functions\n",
    "\n",
    "$h_1(x) = (2x + 1) \\mod{6} $ <br>\n",
    "$h_2(x) = (3x + 2) \\mod{6} $ <br>\n",
    "$h_3(x) = (5x + 2) \\mod{6} $ <br>\n",
    "\n",
    "Now we compute the hash values for $x \\in \\{0,1,2,3,4,5\\}$\n",
    "\n",
    "For $h_1(x) = (2x + 1) \\mod{6} $:\n",
    "\n",
    "$h_1(0) = 1$ <br>\n",
    "$h_1(1) = 3$ <br>\n",
    "$h_1(2) = 5$ <br>\n",
    "$h_1(3) = 1$ <br>\n",
    "$h_1(4) = 3$ <br>\n",
    "$h_1(5) = 5$ <br>\n",
    "\n",
    "For $h_2(x) = (3x + 2) \\mod{6} $:\n",
    "\n",
    "$h_2(0) = 2$ <br>\n",
    "$h_2(1) = 5$ <br>\n",
    "$h_2(2) = 2$ <br>\n",
    "$h_2(3) = 5$ <br>\n",
    "$h_2(4) = 2$ <br>\n",
    "$h_2(5) = 5$ <br>\n",
    "\n",
    "For $h_3(x) = (5x + 2) \\mod{6}$:\n",
    "\n",
    "$h_3(0) = 2$ <br>\n",
    "$h_3(1) = 1$ <br>\n",
    "$h_3(2) = 0$ <br>\n",
    "$h_3(3) = 5$ <br>\n",
    "$h_3(4) = 4$ <br>\n",
    "$h_3(5) = 3$ <br>\n",
    "\n",
    "### MinHash Signature for each set:\n",
    "\n",
    "$S_1 \\{2,5\\}$ (non-zero entries in $S_1$):\n",
    "\n",
    "$h_1(2) = 5, h_1(5) = 5$, $Min = 5$<br>\n",
    "$h_2(2) = 2, h_2(5) = 5$, $Min = 2$<br>\n",
    "$h_3(2) = 0, h_3(5) = 3$, $Min = 0$<br>\n",
    "\n",
    "The same for sets $S_2,S_3,S_4$, the signature table looks like:\n",
    "\n",
    "| Set  | $h_1 Min$ | $h_2 Min$ | $h_3 Min$|\n",
    "|------|-----------|-----------|----------|\n",
    "| $S_1$| 5         | 2         | 0        |\n",
    "| $S_2$| 1         | 2         | 1        |\n",
    "| $S_3$| 1         | 2         | 4        |\n",
    "| $S_4$| 1         | 2         | 0        |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### b)\n",
    "$h_3(x)$ is a true permutation because all hash values $\\{0,1,2,3,4,5\\}$ are distinct.\n",
    "\n",
    "Collisions:\n",
    "\n",
    "$h_1(x)$: <br>\n",
    "$h_1(0) = h_1(3) = 1$ <br>\n",
    "$h_1(1) = h_1(4) = 3$ <br>\n",
    "$h_1(2) = h_1(5) = 5$ <br>\n",
    "\n",
    "$h_2(x)$: <br>\n",
    "$h_2(0) = h_2(2) = h_2(4) = 2$ <br>\n",
    "$h_2(1) = h_2(3) = h_2(5)= 3$ <br>\n",
    "\n",
    "\n",
    "### c)\n",
    "| Pair        | MinHash Similarity | Jaccard Similarity |\n",
    "|-------------|--------------------|--------------------|\n",
    "| $S_1$,$S_2$ | 0.33               | 0                  |\n",
    "| $S_1$,$S_3$ | 0.33               | 0                  |\n",
    "| $S_1$,$S_4$ | 0.67               | 0.33               |\n",
    "| $S_2$,$S_3$ | 0.33               | 0                  |\n",
    "| $S_2$,$S_4$ | 0.67               | 0.33               |\n",
    "| $S_3$,$S_4$ | 0.33               | 0.33               |\n",
    "\n",
    "Due to the small number of hash functions we cannot see that the MinHash similarity would actually converge to the true Jaccard similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1234, 2345, 3456, 4567]\n"
     ]
    }
   ],
   "source": [
    "def compute_k_shingles(digits, k):\n",
    "    # Set to store unique positions\n",
    "    positions = set()\n",
    "\n",
    "    # Iterate over the digits\n",
    "    for i in range(len(digits) - k + 1):\n",
    "        # Get k-shingle at current position\n",
    "        shingle = digits[i:i+k]\n",
    "        # Convert to integer and add to set\n",
    "        positions.add(int(shingle))\n",
    "\n",
    "    # Return ordered list of unique positions\n",
    "    return sorted(positions)\n",
    "\n",
    "# Test function with example\n",
    "test_example = \"1234567\"\n",
    "k = 4\n",
    "shingles = compute_k_shingles(test_example, k)\n",
    "print(shingles)  # Expected: [1234, 2345, 3456, 4567]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpmath import mp\n",
    "\n",
    "# Set precision to 10000 digits\n",
    "mp.dps = 10000\n",
    "\n",
    "# Get pi as string after decimal point\n",
    "pi_digits = str(mp.pi)[2:]\n",
    "\n",
    "# Apply shingles function with k = 12\n",
    "k = 12\n",
    "shingles_positions = compute_k_shingles(pi_digits, k)\n",
    "\n",
    "# Save output to text file\n",
    "with open(\"k_shingles_pi.txt\", \"w\") as f:\n",
    "    for pos in shingles_positions:\n",
    "        f.write(f\"{pos}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinHash Signature: [11610003501, 63680740533, 107687383220, 41635782020, 203614208147]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def minhash_signature(positions, hash_functions):\n",
    "    # Initialize signature\n",
    "    signature = []\n",
    "\n",
    "    # Iterate over hash functions\n",
    "    for a, b, p in hash_functions:\n",
    "        min_hash = float(\"inf\")\n",
    "        # Compute hash value for each position and track the minimum\n",
    "        for pos in positions:\n",
    "            hash_value = ((a * pos + b) % p) % (10**15)\n",
    "            min_hash = min(min_hash, hash_value)\n",
    "        signature.append(min_hash)\n",
    "    return signature\n",
    "\n",
    "def generate_hash_functions():\n",
    "    hash_functions = []\n",
    "    # First hash function\n",
    "    hash_functions.append((37, 126, 10**15 + 223))\n",
    "    \n",
    "    # Generate 4 additional hash functions\n",
    "    for i in [37, 91, 159, 187]:\n",
    "        a = random.randint(0, 10**12)\n",
    "        b = random.randint(0, 10**12)\n",
    "        p = 10**15 + i\n",
    "        hash_functions.append((a, b, p))\n",
    "    \n",
    "    return hash_functions\n",
    "\n",
    "\n",
    "# Generate hash functions\n",
    "hash_functions = generate_hash_functions()\n",
    "\n",
    "# Compute MinHash signature\n",
    "signature = minhash_signature(shingles_positions, hash_functions)\n",
    "\n",
    "# Output the MinHash signature\n",
    "print(\"MinHash Signature:\", signature)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
